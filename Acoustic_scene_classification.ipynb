{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Acoustic scene classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1OMX04mnm0bnE8Kq_2VOPR4xMNTupyiQr",
      "authorship_tag": "ABX9TyOzwvheyQkBiBGOvSmtwoTZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shayanthrn/Acoustic_scene_classification/blob/main/Acoustic_scene_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mA-C-nOrajZ8"
      },
      "source": [
        "#Acoustic scene classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_W5j1vphYZ1a"
      },
      "source": [
        "Dataset: IEEE AASP CASA Challenge, Available on: http://dcase.community/challenge2013/task-acoustic-scene-classification\n",
        "<br/>Currently dataset is in my googe drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5zv_ri5qvFQ"
      },
      "source": [
        "!pip install torchaudio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leigR0aSqRuX"
      },
      "source": [
        "##import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnXHPzXOaoRp"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torchaudio\n",
        "import os\n",
        "from torchvision.transforms import ToTensor\n",
        "import numpy as np"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXJFrOHawk8W"
      },
      "source": [
        "##global variables and hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aipJBiziwpof"
      },
      "source": [
        "class_map = [\"bus\",\"busystreet\",\"office\",\"openairmarket\",\"park\",\"quietstreet\",\"restaurant\",\"supermarket\",\"tube\",\"tubestation\"]\n",
        "BATCH_SIZE = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0LMhgk9q9XW"
      },
      "source": [
        "##create dataset class for DCASE2013 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R6c2UXcq_o2"
      },
      "source": [
        "class DCASE13(Dataset):\n",
        "\n",
        "  def __init__(self,path,class_map):\n",
        "    super().__init__()\n",
        "    self.dataset_path = path\n",
        "    self.class_map = class_map\n",
        "    self.sample_rate = 16000\n",
        "    self.mel_spectrogram = torchaudio.transforms.MelSpectrogram(sample_rate=self.sample_rate,n_fft=1024,hop_length=512,n_mels=64)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(os.listdir(self.dataset_path))\n",
        "  \n",
        "  def __getitem__(self,index):\n",
        "    file_name = os.listdir(self.dataset_path)[index]\n",
        "    class_name = file_name.split('0')[0].split('1')[0]\n",
        "    label = class_map.index(class_name)\n",
        "    file_path = os.path.join(self.dataset_path,file_name)\n",
        "    signal, sample_rate = torchaudio.load(filepath=file_path)\n",
        "    #resample if necessary\n",
        "    if(sample_rate != self.sample_rate):\n",
        "      resampler = torchaudio.transforms.Resample(sample_rate,self.sample_rate)\n",
        "      signal = resampler(signal)\n",
        "    # stereo to mono convert\n",
        "    if(signal.shape[0]>1):\n",
        "      signal = torch.mean(signal, dim=0, keepdim=True) \n",
        "    signal = self.mel_spectrogram(signal)\n",
        "    print(signal.shape)\n",
        "    return signal,label\n",
        "\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PlRuUeH1r2T"
      },
      "source": [
        "dcase13 = DCASE13(\"/content/drive/MyDrive/IEEE_AASP_CASA_Challenge/DCASE13_train\",class_map)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmnJnjd22d2L"
      },
      "source": [
        "dataloader = DataLoader(dcase13 , batch_size=BATCH_SIZE , shuffle=True)"
      ],
      "execution_count": 10,
      "outputs": []
    }
  ]
}